<div align="center">
  <img src="figures/Label.jpg" alt="æ ‡ç­¾" style="display: block; margin: auto; width: 350px; height: 200px;" />
</div>

# LLM/MLLMè®ºæ–‡è®¨è®ºå†…å®¹æ•´ç†
å°†æ¥è§¦åˆ°çš„éƒ¨åˆ†ç§‘ç ”è®¨è®ºè¯é¢˜æ•´ç†å¦‚ä¸‹ï¼Œä»¥ä¾›æ—¥å¸¸ç§‘ç ”å‚è€ƒ

---

## ğŸ“° News
ğŸ˜ å…³äºé¢å‘è¡¨æ ¼æ™ºèƒ½çš„LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ç›¸å…³å·¥ä½œï¼Œè¯¦ç»†æ”¶å½•å¯è§äº[Awesome-Tabular-LLMs](https://github.com/SpursGoZmy/Awesome-Tabular-LLMs)

---

## ğŸš€ Content

### [å¤šæ¨¡æ€å¤§æ¨¡å‹ç›¸å…³](#å¤šæ¨¡æ€å¤§æ¨¡å‹ç›¸å…³)
- [å¤šæ¨¡æ€å¤§æ¨¡å‹ä»‹ç»](#å¤šæ¨¡æ€å¤§æ¨¡å‹ä»‹ç»)
- [å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦é—®é¢˜](#å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦é—®é¢˜)
    - [å¤šæ¨¡æ€å¤§æ¨¡å‹å¹»è§‰](#å¤šæ¨¡æ€å¤§æ¨¡å‹å¹»è§‰)
- [å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦åº”ç”¨](#å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦åº”ç”¨)
    - [å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹](#å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹)

### [å¤§æ¨¡å‹é«˜æ•ˆä¼˜åŒ–ä¸æ‰©å±•](#å¤§æ¨¡å‹é«˜æ•ˆä¼˜åŒ–ä¸æ‰©å±•)
- [å¤§æ¨¡å‹Scaling-Lawsä¸å®ç”¨æ¨å¹¿](#å¤§æ¨¡å‹scaling-lawsä¸å®ç”¨æ¨å¹¿)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆè®­ç»ƒ](#å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆè®­ç»ƒ)
- [å¤§æ¨¡å‹æ˜¾å­˜é«˜æ•ˆè®­ç»ƒ](#å¤§æ¨¡å‹æ˜¾å­˜é«˜æ•ˆè®­ç»ƒ)
- [å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ](#å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ)

### [å¤§æ¨¡å‹çŸ¥è¯†å¤„ç†](#å¤§æ¨¡å‹çŸ¥è¯†å¤„ç†)
- [å¤§æ¨¡å‹çŸ¥è¯†æ¨ç†](#å¤§æ¨¡å‹çŸ¥è¯†æ¨ç†)
- [å¤§æ¨¡å‹çŸ¥è¯†ç¼–è¾‘](#å¤§æ¨¡å‹çŸ¥è¯†ç¼–è¾‘)
- [å¤§æ¨¡å‹çŸ¥è¯†è’¸é¦](#å¤§æ¨¡å‹çŸ¥è¯†è’¸é¦)
- [å¤§æ¨¡å‹å¯è§£é‡Šæ€§](#å¤§æ¨¡å‹å¯è§£é‡Šæ€§)
- [å¤§æ¨¡å‹å¹»è§‰](#å¤§æ¨¡å‹å¹»è§‰)

### [å¤§æ¨¡å‹æ–‡æœ¬çš„å¤„ç†ä¸å¯¹é½](#å¤§æ¨¡å‹æ–‡æœ¬çš„å¤„ç†ä¸å¯¹é½)
- [å¤§æ¨¡å‹çš„é•¿æ–‡æœ¬å¤„ç†](#å¤§æ¨¡å‹çš„é•¿æ–‡æœ¬å¤„ç†)
- [å¤§æ¨¡å‹çš„äººç±»åå¥½å¯¹é½](#å¤§æ¨¡å‹çš„äººç±»åå¥½å¯¹é½)
- [å¤§æ¨¡å‹æ–‡æœ¬æ°´å°æŠ€æœ¯](#å¤§æ¨¡å‹æ–‡æœ¬æ°´å°æŠ€æœ¯)

### [å¤§æ¨¡å‹æ¶æ„](#å¤§æ¨¡å‹æ¶æ„)

---

## å¤šæ¨¡æ€å¤§æ¨¡å‹ç›¸å…³
### å¤šæ¨¡æ€å¤§æ¨¡å‹ä»‹ç»
Coming Soon...

### å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦é—®é¢˜
Coming Soon...

### å¤šæ¨¡æ€å¤§æ¨¡å‹å¹»è§‰

> **å¤šæ¨¡æ€å¹»è§‰**é€šå¸¸æŒ‡"The discrepancy between generated text response and provided visual content"ã€‚ç®€å•åœ°è¯´ï¼ŒæŒ‡æ¨¡æ€ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚ï¼ˆå¯¼è‡´å¿½ç•¥äº†äº‹å®æ­£ç¡®æ€§ï¼‰ä¾‹å¦‚ï¼Œåœ¨å›¾åƒæ ‡é¢˜ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç”Ÿæˆçš„æ ‡é¢˜åŒ…å«äº†å›¾åƒä¸­ä¸å­˜åœ¨çš„äº‹ç‰©ã€‚

> ç°é˜¶æ®µï¼Œå¤šæ¨¡æ€å¹»è§‰å¯è¢«åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š

> - 1.Category(ç±»åˆ«å¹»è§‰):æ¨¡å‹â€œå‘ç°â€äº†å›¾ä¸­ä¸å­˜åœ¨çš„ç‰©ä½“

> - 2.Attribute(å±æ€§å¹»è§‰):æ¨¡å‹å¯¹äºå›¾ä¸­çš„ç‰©ä½“çš„å±æ€§(å±æ€§åŒ…æ‹¬ï¼Œå½¢çŠ¶ï¼Œé¢œè‰²ï¼Œæ•°é‡ç­‰)æœ‰ç€é”™è¯¯çš„ç†è§£

> - 3.Relation(å…³ç³»å¹»è§‰):æ¨¡å‹å¯¹äºå›¾ä¸­ç‰©ä½“çš„å…³ç³»æ²¡æœ‰æ­£ç¡®ç†è§£ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹å°†èŒ¶å‡ å’Œæ²™å‘çš„å…³ç³»ç†è§£ä¸ºèŒ¶å‡ æ”¾åœ¨æ²™å‘ä¸Š

> æ›´å¤šåŸºç¡€çŸ¥è¯†å¯è§äº[(å‚è€ƒç»¼è¿°)](https://arxiv.org/abs/2404.18930)

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:å¤šæ¨¡æ€å¹»è§‰**

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **Hallucination of Multimodal Large Language Models: A Survey** [[paper]](https://arxiv.org/abs/2404.18930)[[project]](https://github.com/showlab/Awesome-MLLM-Hallucination) | ç»¼è¿° | arxiv | 2024-04-29 |
| **A Survey on Hallucination in Large Vision-Language Models**  [[paper]](https://arxiv.org/abs/2402.00253)[[project]](https://github.com/lhanchao777/LVLM-Hallucinations-Survey) | ç»¼è¿° | arxiv | 2024-05-06 |
| **Evaluating Object Hallucination in Large Vision-Language Models** [[paper]](https://arxiv.org/abs/2305.10355)[[project]](https://github.com/RUCAIBox/POPE) | å¤šæ¨¡æ€å¹»è§‰è¯„ä¼° | EMNLP2023 | 2023-05-17 |
| **MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models** [[paper]](https://arxiv.org/abs/2306.13394)[[project]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) | å¤šæ¨¡æ€å¹»è§‰è¯„ä¼° | arxiv | 2023-06-23 |
| **Aligning Large Multimodal Models with Factually Augmented RLHF** [[paper]](https://arxiv.org/abs/2309.14525)[[project]](https://llava-rlhf.github.io/) | å¤šæ¨¡æ€å¹»è§‰ç¼“è§£,å¤šæ¨¡æ€å¹»è§‰è¯„ä¼° | arxiv | 2023-09-25 |
| **OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation** [[paper]](https://arxiv.org/abs/2311.17911)[[project]](https://github.com/shikiw/OPERA) | å¤šæ¨¡æ€å¹»è§‰ç¼“è§£ | **CVPR 2024 Highlight** | 2023-09-29 |
| **Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective** [[paper]](https://arxiv.org/pdf/2402.14545)[[project]](https://github.com/yuezih/less-is-more) | å¤šæ¨¡æ€å¹»è§‰ç¼“è§£ | ACL 2024 | 2024-02-22 |


### å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦åº”ç”¨
Coming Soon...

#### å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹
Coming Soon...

## å¤§æ¨¡å‹é«˜æ•ˆä¼˜åŒ–ä¸æ‰©å±•
### å¤§æ¨¡å‹Scaling-Lawsä¸å®ç”¨æ¨å¹¿

+ `åŸºç¡€çŸ¥è¯†`:

> + **Scaling Laws** æ˜¯æŒ‡æ¨¡å‹æ€§èƒ½ï¼ˆå¦‚æŸå¤±ã€ç²¾åº¦ï¼‰ä¸æ¨¡å‹è§„æ¨¡ï¼ˆå¦‚å‚æ•°æ•°é‡ $N$ ã€è®­ç»ƒæ•°æ®é‡ $D$ ã€è®¡ç®—èµ„æº $C$ ï¼‰ä¹‹é—´çš„ç»éªŒæ€§å…³ç³»ã€‚å…³é”®è¦ç‚¹åŒ…æ‹¬ï¼š

> 1. **å‚æ•°æ•°é‡ $N$**ï¼šæ¨¡å‹æ€§èƒ½éšå‚æ•°å¢åŠ æå‡ï¼Œä½†å¢å¹…é€æ¸å‡å°ã€‚
> 2. **æ•°æ®é‡ $D$**ï¼šæ›´å¤šæ•°æ®èƒ½æå‡å¤§æ¨¡å‹æ€§èƒ½ã€‚
> 3. **è®¡ç®—é‡ $C$**ï¼šæ›´å¤šè®¡ç®—èµ„æºèƒ½å¸¦æ¥æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚
> 4. **æŸå¤±å‡½æ•° $L$**ï¼šéšç€èµ„æºæŠ•å…¥å¢åŠ ï¼ŒæŸå¤±å‡½æ•°å‘ˆç°æ¬¡å¹‚ç¼©å‡è¶‹åŠ¿ã€‚
> 5. **åº”ç”¨**ï¼šScaling Laws ä¸»è¦ç”¨äºæŒ‡å¯¼æ¨¡å‹çš„è®¾è®¡ä¸è®­ç»ƒï¼Œä½¿å¾—èµ„æºçš„æŠ•å…¥èƒ½å¤Ÿå¸¦æ¥æœ€ä½³çš„æ€§èƒ½æå‡ã€‚æ¯”å¦‚é€šè¿‡ Scaling Lawsï¼Œç ”ç©¶äººå‘˜å¯ä»¥é¢„ä¼°å¢åŠ å‚æ•°ã€æ•°æ®æˆ–è®¡ç®—æ˜¯å¦ä¼šæœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œä»è€Œæ›´åˆç†åœ°åˆ†é…èµ„æºï¼Œæ¨åŠ¨å¼€å‘æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚

> + Scaling Lawsä¸ä»…æ˜¯ä¸€ç§é’ˆå¯¹å¤§æ¨¡å‹è®­ç»ƒçš„ç†è®ºç§¯ç´¯ï¼Œè¿˜æ˜¯ä¸€ç§å®è·µæ–¹æ³•ï¼Œå¯ä»¥è¿ç§»åˆ°å„ç§éœ€è¦ä¼°è®¡æ•°æ®é‡ã€æ•°æ®æ¯”ä¾‹ã€å‚æ•°é‡ï¼ˆæˆ–å…¶ä»–æ¦‚å¿µï¼‰çš„å¤§æ¨¡å‹è®­ç»ƒåœºæ™¯ï¼ˆå³ï¼Œæ‰¾è‡ªå·±ç ”ç©¶åœºæ™¯é‡Œçš„Scaling Lawsï¼‰ã€‚

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:Scaling Lawsä¸å®ç”¨æ¨å¹¿**


| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **Scaling laws for neural language models** [[paper]](arxiv.org/abs/2001.08361) | Scaling Laws | OpenAIæŠ€æœ¯æŠ¥å‘Š | 2020-01-23 |
| **Training Compute-Optimal Large Language Models** [[paper]](https://arxiv.org/abs/2203.15556) | æœ‰å…³å¤§æ¨¡å‹å‚æ•°é‡ï¼Œæ•°æ®é‡å’Œè®¡ç®—é‡çš„Scaling Laws | NIPS2022 | 2022-03-29 |
| **Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies** [[paper]](https://arxiv.org/abs/2407.13623) [[project]](https://github.com/sail-sg/scaling-with-vocab) | æœ‰å…³å¤§æ¨¡å‹è¯è¡¨çš„Scaling Laws | arxiv | 2024-07-18 |
| **D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models** [[paper]](https://arxiv.org/abs/2406.01375) | Continual Pre-Trainingçš„Scaling Laws | arxiv | 2024-06-03 |
| **Observational Scaling Laws and the Predictability of Language Model Performance** [[paper]](https://arxiv.org/abs/2405.10938) [[project]](https://github.com/ryoungj/ObsScaling)| åœ¨å¤šä¸ªModel Familyï¼Œå¤šä¸ªBenchmarkä¸Šæ€»ç»“çš„é€šç”¨Scaling Laws | arxiv | 2024-05-17 |



### å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆè®­ç»ƒ

+ `åŸºç¡€çŸ¥è¯†`:

>  + **æ¦‚å¿µç®€ä»‹**:ç”±äºå¤§è¯­è¨€æ¨¡å‹å‚æ•°é‡ååˆ†åºå¤§ï¼Œå½“å°†å…¶åº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œå¾®è°ƒå…¨éƒ¨å‚æ•°éœ€è¦ç›¸å½“é«˜çš„ç®—åŠ›ã€‚ä¸ºäº†èŠ‚çœæˆæœ¬ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†å¤šç§å‚æ•°é«˜æ•ˆï¼ˆParameter Efficientï¼‰çš„å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨ä»…è®­ç»ƒå°‘é‡å‚æ•°ä½¿æ¨¡å‹é€‚åº”åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚

>  + **å‚æ•°é«˜æ•ˆæ–¹æ³•åˆ†ç±»**:å½“å‰åº”ç”¨æœ€é¢‘ç¹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å½“å±LoRAåŠå…¶å˜ä½“(å¦‚QLoRA,AdaLoRAç­‰)ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå…¸å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•è¿˜æœ‰åŸºäºAdapterçš„æ–¹æ³•(å¦‚[Adapter tuning](https://arxiv.org/pdf/1902.00751))å’ŒåŸºäºå‰ç¼€å¾®è°ƒçš„æ–¹æ³•(å¦‚[Prefix Tuning](https://arxiv.org/abs/2101.00190))


>  + **å…¸å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ä¸¾ä¾‹â€”â€”**[LoRA](https://arxiv.org/abs/2106.09685),æ˜¯å½“å‰è¢«å¹¿æ³›åº”ç”¨äºLLMè®­ç»ƒçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚LoRAæ–¹æ³•æµç¨‹ä¸º:å›ºå®šé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ä¸å˜ï¼Œåœ¨åŸæœ¬æƒé‡çŸ©é˜µæ—è·¯æ·»åŠ ä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ä½œä¸ºå¯è®­ç»ƒå‚æ•°(è§ä¸‹å›¾),ç”¨ä»¥æ¨¡æ‹Ÿæ¨¡å‹å‚æ•°çš„å˜åŒ–é‡ã€‚

>  + æ¨¡å‹å‚æ•°å˜åŒ–é‡çš„è®¡ç®—å…¬å¼ä¸º:  $W = W_0 + B \cdot A$  (å…¶ä¸­ $B$ , $A$ ä¸ºä½ç§©çŸ©é˜µã€‚åˆå§‹åŒ–æ—¶ï¼ŒçŸ©é˜µ $A$ é€šè¿‡é«˜æ–¯å‡½æ•°åˆå§‹åŒ–ï¼ŒçŸ©é˜µ $B$ ä¸ºé›¶åˆå§‹åŒ–ï¼Œä½¿å¾—è®­ç»ƒå¼€å§‹ä¹‹å‰æ—è·¯å¯¹åŸæ¨¡å‹ä¸é€ æˆå½±å“)

<div align="center">
  <img src="figures/LoRA.jpg" alt="LoRA" width="500"><br>
</div></br>

>  + LoRAåœ¨å®é™…å¾®è°ƒå¤§æ¨¡å‹æ—¶å…·æœ‰è‰¯å¥½æ•ˆæœï¼Œå¹¶ä¸”ç”±äºå‚æ•°åˆå¹¶ï¼Œä¸ä¼šå¸¦æ¥é¢å¤–æ¨ç†æ—¶å»¶ã€‚åç»­çš„ç›¸å…³å·¥ä½œä¹Ÿå°è¯•æ”¹è¿›LoRAï¼Œå°†å…¶åº”ç”¨åˆ°æ›´å¹¿æ³›çš„å®é™…ä»»åŠ¡ä¸­ã€‚

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:å¤ç”¨å’Œç»„åˆLoRAæ¨¡ç»„ for few/zero shots learning**


| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning** [[paper]](https://arxiv.org/abs/2205.05638)[[project]](https://github.com/r-three/t-few) | Adapter Tuning, å¤šä»»åŠ¡å­¦ä¹  | NIPS2022 | 2022-08-26 |
| **Combining Parameter-efficient Modules for Task-level Generalisation** [[paper]](https://aclanthology.org/2023.eacl-main.49/)[[project]](https://github.com/microsoft/mttl) | LoRAç»„åˆ, å¤šä»»åŠ¡å­¦ä¹  | EACL2023 | 2023 |
| **LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition** [[paper]](https://arxiv.org/abs/2307.13269)[[project]](https://huggingface.co/lorahub) | LoRAç»„åˆ, å¤šä»»åŠ¡å­¦ä¹  | COLM2024 | 2023-07-25 |
| **Towards Modular LLMs by Building and Reusing a Library of LoRAs** [[paper]](https://arxiv.org/abs/2405.11157)| LoRAç»„åˆ, å¤šä»»åŠ¡å­¦ä¹  | ICML2024 | 2024-05-18 |
| **Learning to Route Among Specialized Experts for Zero-Shot Generalization** [[paper]](https://arxiv.org/abs/2402.05859) [[project]](https://github.com/r-three/phatgoose)| LoRAç»„åˆ, å¤šä»»åŠ¡å­¦ä¹  | ICML2024 | 2024-02-08 |


### å¤§æ¨¡å‹æ˜¾å­˜é«˜æ•ˆè®­ç»ƒ
Coming Soon...

### å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ

+ `åŸºç¡€çŸ¥è¯†`:

>  + **æ¦‚å¿µç®€ä»‹**:å¤§è¯­è¨€æ¨¡å‹çš„è½åœ°åº”ç”¨å—åˆ°å…¶è¾ƒå¤§çš„æ¨ç†å¼€é”€çš„é™åˆ¶ï¼Œå¯¹éƒ¨ç½²èµ„æºã€ç”¨æˆ·ä½“éªŒã€ç»æµæˆæœ¬éƒ½å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œå°†LLaMA-2-70Bæ¨¡å‹è¿›è¡Œéƒ¨ç½²æ¨ç†ï¼Œè‡³å°‘éœ€è¦6å¼ RTX 3090Tiæ˜¾å¡æˆ–2å¼ NVIDIA A100æ˜¾å¡ï¼Œä»¥éƒ¨ç½²åœ¨A100æ˜¾å¡ä¸Šä¸ºä¾‹ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆ512é•¿åº¦çš„è¯å—ï¼ˆtokenï¼‰åºåˆ—éœ€è¦è€—æ—¶è¶…è¿‡50ç§’ã€‚å› æ­¤ï¼Œè®¾è®¡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼€é”€çš„æŠ€æœ¯ï¼Œæˆä¸ºè®¸å¤šç ”ç©¶çš„é‡è¦ç›®æ ‡ã€‚

>  + å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…éƒ¨ç½²åº”ç”¨ä¸­ï¼Œäººä»¬é€šå¸¸å…³æ³¨å…¶**å»¶æ—¶ã€ååã€åŠŸè€—å’Œå­˜å‚¨**ï¼Œè€Œåœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸‰ä¸ªé‡è¦å› ç´ ä¼šç›´æ¥å½±å“ä¸Šè¿°æ•ˆç‡æŒ‡æ ‡ï¼Œåˆ†åˆ«æ˜¯**è®¡ç®—å¼€é”€ï¼ˆComputational Costï¼‰**ã€**è®¿å­˜å¼€é”€ï¼ˆMemory Access Costï¼‰**å’Œ**å­˜å‚¨å¼€é”€ï¼ˆMemory Costï¼‰**ã€‚[ç°æœ‰ç»¼è¿°](https://arxiv.org/abs/2404.14294)æ€»ç»“å‡ºå½±å“ä¸Šè¿°æŒ‡æ ‡çš„ä¸‰ç‚¹æ ¹æœ¬å› ç´ ï¼Œåˆ†åˆ«ä¸ºï¼š

> 1. **æ¨¡å‹è§„æ¨¡**ï¼šä¸»æµå¤§è¯­è¨€æ¨¡å‹åºå¤§çš„æ¨¡å‹è§„æ¨¡ä¼šå¯¼è‡´å·¨å¤§çš„è®¡ç®—é‡ã€è®¿å­˜é‡å’Œå­˜å‚¨é‡ã€‚
> 2. **æ³¨æ„åŠ›ç®—å­**ï¼šä½œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒç®—å­ï¼Œæ³¨æ„åŠ›ç®—å­å…·æœ‰ä¸è¾“å…¥é•¿åº¦å‘ˆå¹³æ–¹å…³ç³»å¢é•¿çš„è®¡ç®—å’Œå­˜å‚¨å¤æ‚åº¦ã€‚
> 3. **è§£ç æ–¹å¼**ï¼šä¸»æµçš„è‡ªå›å½’è§£ç æ–¹å¼å¯¼è‡´æä½çš„è®¡ç®—-è®¿å­˜æ¯”å’Œç¡¬ä»¶åˆ©ç”¨ç‡ï¼ŒåŒæ—¶åŠ¨æ€å¢é•¿çš„KV cache[(å¦‚ä¸äº†è§£KV cacheï¼Œå¯å‚è€ƒæ­¤ç®€ä»‹)](https://zhuanlan.zhihu.com/p/662498827)ä¼šå¯¼è‡´ç¢ç‰‡åŒ–çš„å†…å­˜ä½¿ç”¨ï¼Œå¯¹è®¿å­˜å¼€é”€å’Œå­˜å‚¨å¼€é”€å¸¦æ¥å¢é•¿ã€‚

>  + é’ˆå¯¹ä¸Šè¿°å› ç´ ï¼Œä»æ¨ç†é«˜æ•ˆè§’åº¦å‡ºå‘çš„ç°æœ‰æŠ€æœ¯å¯å¤§è‡´åˆ†ä¸ºä¸‰ç±»[(åˆ†ç±»ä¾æ®)](https://arxiv.org/abs/2404.14294):

> 1. **æ•°æ®å±‚ä¼˜åŒ–æŠ€æœ¯**ï¼šï¼ˆæŠ€æœ¯å…¸å‹:Promptå‹ç¼©ï¼‰æŒ‡é€šè¿‡ä¼˜åŒ–è¾“å…¥æç¤ºè¯æˆ–è§„åˆ’æ¨¡å‹è¾“å‡ºå†…å®¹ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚è¿™ç±»ä¼˜åŒ–æŠ€æœ¯é€šå¸¸ä¸éœ€è¦ä¿®æ”¹æ¨¡å‹æœ¬èº«ï¼Œå› æ­¤é¿å…äº†å¤§é‡çš„æ¨¡å‹è®­ç»ƒæˆ–å¾®è°ƒå¼€é”€ï¼›
> 2. **æ¨¡å‹å±‚ä¼˜åŒ–æŠ€æœ¯**ï¼šï¼ˆæŠ€æœ¯å…¸å‹:æ¨¡å‹å‰ªæï¼‰æŒ‡é€šè¿‡è®¾è®¡é«˜æ•ˆçš„æ¨¡å‹ç»“æ„æˆ–æ¨¡å‹å‹ç¼©æŠ€æœ¯ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚è¿™ç±»æŠ€æœ¯é€šå¸¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒæˆ–å¾®è°ƒæ¥æ¢å¤ä»»åŠ¡ç²¾åº¦ï¼ŒåŒæ—¶é€šå¸¸å¯¹è¾“å‡ºç»“æœæ˜¯æœ‰æŸçš„ï¼›
> 3. **ç³»ç»Ÿå±‚ä¼˜åŒ–æŠ€æœ¯**ï¼šï¼ˆæŠ€æœ¯å…¸å‹:ç®—å­ä¼˜åŒ–ï¼ŒçŒœæµ‹è§£ç ï¼‰æŒ‡é€šè¿‡ä¼˜åŒ–æ¨ç†å¼•æ“æˆ–æœåŠ¡ç³»ç»Ÿä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚è¿™ç±»æŠ€æœ¯é€šå¸¸ä¸éœ€è¦é¢å¤–çš„æ¨¡å‹è®­ç»ƒå¼€é”€ï¼ŒåŒæ—¶å¯ä»¥ä¿è¯å¯¹è¾“å‡ºç»“æœæ˜¯æ— æŸçš„ã€‚

<div align="center">
  <img src="figures/LLM_efficient_inference.png" alt="LLM_efficient_inference" width="500"><br>
</div></br>

> æ›´å¤šåŸºç¡€çŸ¥è¯†å¯è§äº[(å‚è€ƒç»¼è¿°)](https://arxiv.org/abs/2404.14294)

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:ä½¿ç”¨è¶…ç½‘ç»œç”ŸæˆPEFTæ¨¡å—(æ¶‰åŠå†…å®¹å½’å±ï¼šæ•°æ®å±‚ä¼˜åŒ–æŠ€æœ¯)**
> + **è¯é¢˜ç®€ä»‹**:è¶…ç½‘ç»œï¼ˆHypernetworksï¼‰è¡¨ç¤ºç”¨äºäº§ç”Ÿç½‘ç»œå‚æ•°çš„ç½‘ç»œã€‚LLMæ¨ç†é€Ÿåº¦å—é™äºå†—é•¿çš„æŒ‡ä»¤å’Œå°‘æ ·æœ¬ç¤ºä¾‹ã€‚ç”¨è¶…ç½‘ç»œä¸ºinstruction/few-shot demonstrationç”ŸæˆPEFTæ¨¡å—,åˆ™æ— éœ€æ¯æ¬¡å¤„ç†è¾“å…¥promptã€‚ï¼ˆéœ€æ³¨æ„ï¼Œæ­¤è¯é¢˜ä¸‹çš„æŠ€æœ¯å¯ä»¥æŒ‰å¤šç§ç†è®ºè§†è§’è§£è¯»ï¼Œæ—¢å¯ä»¥è§†ä¸ºä¸€ç§èƒ½å¢ç›Šå¤§æ¨¡å‹æ¨ç†æ•ˆç‡çš„soft promptæŠ€æœ¯ï¼Œä¹Ÿå¯ä»¥è§†ä¸ºä¸€ç§å‚æ•°é«˜æ•ˆçš„å¤§æ¨¡å‹ä¼˜åŒ–æ–¹æ³•ã€‚ï¼‰

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **HyperPrompt: Prompt-based Task-Conditioning of Transformers** [[paper]](https://arxiv.org/abs/2203.00759) | å¤šä»»åŠ¡å­¦ä¹  | ICML2022 | 2022-03-01 |
| **Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning** [[paper]](https://arxiv.org/abs/2310.11670)[[project]](https://github.com/Bumble666/PHA) | å¤šä»»åŠ¡å­¦ä¹  | EMNLP2023 | 2023-10-18 |
| **HyperTuning: Toward Adapting Large Language Models without Back-propagation** [[paper]](https://arxiv.org/abs/2211.12485)| å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆè®­ç»ƒ | ICML2023 | 2022-11-22 |
| **HINT: Hypernetwork Instruction Tuning for Efficient Zero- & Few-Shot Generalisation** [[paper]](https://arxiv.org/abs/2212.10315)[[project]](https://github.com/allenai/hyper-task-descriptions) | å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ | ACL2023 | 2022-12-20 |

+ **è¯é¢˜:ä»ç¨€ç–æ€§è§’åº¦çœ‹LLMæ¨ç†åŠ é€Ÿ(æ¶‰åŠå†…å®¹å½’å±ï¼šæ¨¡å‹å±‚ä¼˜åŒ–æŠ€æœ¯ï¼Œç³»ç»Ÿå±‚ä¼˜åŒ–æŠ€æœ¯)**

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time** [[paper]](https://arxiv.org/abs/2310.17157)[[project]](https://github.com/FMInference/DejaVu) | å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ | ICML2023 | 2023-10-26 |
| **Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning** [[paper]](https://arxiv.org/abs/2310.06694)[[project]](https://github.com/princeton-nlp/LLM-Shearing) | å¤§æ¨¡å‹å‰ªæ | ICLR2024 | 2023-10-10 |
| **Fluctuation-based Adaptive Structured Pruning for Large Language Models** [[paper]](https://arxiv.org/abs/2312.11983)[[project]](https://github.com/CASIA-IVA-Lab/FLAP) | å¤§æ¨¡å‹å‰ªæ | AAAI2024 | 2023-12-19 |
| **PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs** [[paper]](https://arxiv.org/abs/2312.15230)[[project]](https://github.com/ZIB-IOL/PERP) | å¤§æ¨¡å‹å‰ªæ | arxiv | 2023-12-23 |
| **LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training** [[paper]](https://arxiv.org/abs/2406.16554)[[project]](https://github.com/pjlab-sys4nlp/llama-moe) | LLM MoE | arxiv | 2024-06-24 |


## å¤§æ¨¡å‹çŸ¥è¯†å¤„ç†
### å¤§æ¨¡å‹çŸ¥è¯†æ¨ç†
Coming Soon...

### å¤§æ¨¡å‹çŸ¥è¯†ç¼–è¾‘
Coming Soon...

### å¤§æ¨¡å‹çŸ¥è¯†è’¸é¦
Coming Soon...

### å¤§æ¨¡å‹å¯è§£é‡Šæ€§
Coming Soon...

### å¤§æ¨¡å‹å¹»è§‰
Coming Soon...

## å¤§æ¨¡å‹çš„æ–‡æœ¬å¤„ç†ä¸å¯¹é½
### å¤§æ¨¡å‹çš„è¡¨æ ¼å¤„ç†

### å¤§æ¨¡å‹çš„é•¿æ–‡æœ¬å¤„ç†
Coming Soon...

### å¤§æ¨¡å‹çš„äººç±»åå¥½å¯¹é½
Coming Soon...

### å¤§æ¨¡å‹æ–‡æœ¬æ°´å°æŠ€æœ¯
Coming Soon...

## å¤§æ¨¡å‹æ¶æ„
Coming Soon...