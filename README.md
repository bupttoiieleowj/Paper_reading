<div align="center">
  <img src="figures/Label.jpg" alt="æ ‡ç­¾" style="display: block; margin: auto; width: 400px; height: 200px;" />
</div>

# LLM/MLLMè®ºæ–‡è®¨è®ºå†…å®¹æ•´ç†
å°†æ¥è§¦åˆ°çš„éƒ¨åˆ†ç§‘ç ”è®¨è®ºè¯é¢˜æ•´ç†å¦‚ä¸‹ï¼Œä»¥ä¾›æ—¥å¸¸ç§‘ç ”å‚è€ƒ

---

## ğŸ“° News
ğŸ˜ å…³äºé¢å‘è¡¨æ ¼æ™ºèƒ½çš„LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ç›¸å…³å·¥ä½œï¼Œè¯¦ç»†æ”¶å½•å¯è§äº[Awesome-Tabular-LLMs](https://github.com/SpursGoZmy/Awesome-Tabular-LLMs)

---

## ğŸš€ Content

### [å¤šæ¨¡æ€å¤§æ¨¡å‹ç›¸å…³](#å¤šæ¨¡æ€å¤§æ¨¡å‹ç›¸å…³)
- [å¤šæ¨¡æ€å¤§æ¨¡å‹ä»‹ç»](#å¤šæ¨¡æ€å¤§æ¨¡å‹ä»‹ç»)
- [å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦é—®é¢˜](#å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦é—®é¢˜)
    - [å¤šæ¨¡æ€å¤§æ¨¡å‹å¹»è§‰](#å¤šæ¨¡æ€å¤§æ¨¡å‹å¹»è§‰)
- [å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦åº”ç”¨](#å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦åº”ç”¨)
    - [å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹](#å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹)

### [å¤§æ¨¡å‹é«˜æ•ˆä¼˜åŒ–ä¸æ‰©å±•](#å¤§æ¨¡å‹é«˜æ•ˆä¼˜åŒ–ä¸æ‰©å±•)
- [å¤§æ¨¡å‹Scaling-Lawsä¸å®ç”¨æ¨å¹¿](#å¤§æ¨¡å‹scaling-lawsä¸å®ç”¨æ¨å¹¿)
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆè®­ç»ƒ](#å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆè®­ç»ƒ)
- [å¤§æ¨¡å‹æ˜¾å­˜é«˜æ•ˆè®­ç»ƒ](#å¤§æ¨¡å‹æ˜¾å­˜é«˜æ•ˆè®­ç»ƒ)
- [å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ](#å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ)

### [å¤§æ¨¡å‹çŸ¥è¯†å¤„ç†](#å¤§æ¨¡å‹çŸ¥è¯†å¤„ç†)
- [å¤§æ¨¡å‹çŸ¥è¯†æ¨ç†](#å¤§æ¨¡å‹çŸ¥è¯†æ¨ç†)
- [å¤§æ¨¡å‹çŸ¥è¯†ç¼–è¾‘](#å¤§æ¨¡å‹çŸ¥è¯†ç¼–è¾‘)
- [å¤§æ¨¡å‹çŸ¥è¯†è’¸é¦](#å¤§æ¨¡å‹çŸ¥è¯†è’¸é¦)
- [å¤§æ¨¡å‹å¯è§£é‡Šæ€§](#å¤§æ¨¡å‹å¯è§£é‡Šæ€§)
- [å¤§æ¨¡å‹å¹»è§‰](#å¤§æ¨¡å‹å¹»è§‰)

### [å¤§æ¨¡å‹æ–‡æœ¬çš„å¤„ç†ä¸å¯¹é½](#å¤§æ¨¡å‹æ–‡æœ¬çš„å¤„ç†ä¸å¯¹é½)
- [å¤§æ¨¡å‹çš„è¡¨æ ¼å¤„ç†](#å¤§æ¨¡å‹çš„è¡¨æ ¼å¤„ç†)
- [å¤§æ¨¡å‹çš„é•¿æ–‡æœ¬å¤„ç†](#å¤§æ¨¡å‹çš„é•¿æ–‡æœ¬å¤„ç†)
- [å¤§æ¨¡å‹çš„äººç±»åå¥½å¯¹é½](#å¤§æ¨¡å‹çš„äººç±»åå¥½å¯¹é½)
- [å¤§æ¨¡å‹æ–‡æœ¬æ°´å°æŠ€æœ¯](#å¤§æ¨¡å‹æ–‡æœ¬æ°´å°æŠ€æœ¯)

### [å¤§æ¨¡å‹æ¶æ„](#å¤§æ¨¡å‹æ¶æ„)

---

## å¤šæ¨¡æ€å¤§æ¨¡å‹ç›¸å…³
### å¤šæ¨¡æ€å¤§æ¨¡å‹ä»‹ç»
Coming Soon...

### å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦é—®é¢˜
Coming Soon...

### å¤šæ¨¡æ€å¤§æ¨¡å‹å¹»è§‰

> **å¤šæ¨¡æ€å¹»è§‰**é€šå¸¸æŒ‡"The discrepancy between generated text response and provided visual content"ã€‚ç®€å•åœ°è¯´ï¼ŒæŒ‡æ¨¡æ€ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚ï¼ˆå¯¼è‡´å¿½ç•¥äº†äº‹å®æ­£ç¡®æ€§ï¼‰ä¾‹å¦‚ï¼Œåœ¨å›¾åƒæ ‡é¢˜ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç”Ÿæˆçš„æ ‡é¢˜åŒ…å«äº†å›¾åƒä¸­ä¸å­˜åœ¨çš„äº‹ç‰©ã€‚

> ç°é˜¶æ®µï¼Œå¤šæ¨¡æ€å¹»è§‰å¯è¢«åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š

> - 1.Category(ç±»åˆ«å¹»è§‰):æ¨¡å‹â€œå‘ç°â€äº†å›¾ä¸­ä¸å­˜åœ¨çš„ç‰©ä½“

> - 2.Attribute(å±æ€§å¹»è§‰):æ¨¡å‹å¯¹äºå›¾ä¸­çš„ç‰©ä½“çš„å±æ€§(å±æ€§åŒ…æ‹¬ï¼Œå½¢çŠ¶ï¼Œé¢œè‰²ï¼Œæ•°é‡ç­‰)æœ‰ç€é”™è¯¯çš„ç†è§£

> - 3.Relation(å…³ç³»å¹»è§‰):æ¨¡å‹å¯¹äºå›¾ä¸­ç‰©ä½“çš„å…³ç³»æ²¡æœ‰æ­£ç¡®ç†è§£ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹å°†èŒ¶å‡ å’Œæ²™å‘çš„å…³ç³»ç†è§£ä¸ºèŒ¶å‡ æ”¾åœ¨æ²™å‘ä¸Š

> æ›´å¤šåŸºç¡€çŸ¥è¯†å¯è§äº[(å‚è€ƒç»¼è¿°)](https://arxiv.org/abs/2404.18930)

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:å¤šæ¨¡æ€å¹»è§‰**

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **Hallucination of Multimodal Large Language Models: A Survey** [[paper]](https://arxiv.org/abs/2404.18930)[[project]](https://github.com/showlab/Awesome-MLLM-Hallucination) | ç»¼è¿° | arxiv | 2024-04-29 |
| **A Survey on Hallucination in Large Vision-Language Models**  [[paper]](https://arxiv.org/abs/2402.00253)[[project]](https://github.com/lhanchao777/LVLM-Hallucinations-Survey) | ç»¼è¿° | arxiv | 2024-05-06 |
| **Evaluating Object Hallucination in Large Vision-Language Models** [[paper]](https://arxiv.org/abs/2305.10355)[[project]](https://github.com/RUCAIBox/POPE) | å¤šæ¨¡æ€å¹»è§‰è¯„ä¼° | EMNLP2023 | 2023-05-17 |
| **MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models** [[paper]](https://arxiv.org/abs/2306.13394)[[project]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) | å¤šæ¨¡æ€å¹»è§‰è¯„ä¼° | arxiv | 2023-06-23 |
| **Aligning Large Multimodal Models with Factually Augmented RLHF** [[paper]](https://arxiv.org/abs/2309.14525)[[project]](https://llava-rlhf.github.io/) | å¤šæ¨¡æ€å¹»è§‰ç¼“è§£,å¤šæ¨¡æ€å¹»è§‰è¯„ä¼° | arxiv | 2023-09-25 |
| **OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation** [[paper]](https://arxiv.org/abs/2311.17911)[[project]](https://github.com/shikiw/OPERA) | å¤šæ¨¡æ€å¹»è§‰ç¼“è§£ | **CVPR 2024 Highlight** | 2023-09-29 |
| **Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective** [[paper]](https://arxiv.org/pdf/2402.14545)[[project]](https://github.com/yuezih/less-is-more) | å¤šæ¨¡æ€å¹»è§‰ç¼“è§£ | ACL 2024 | 2024-02-22 |


### å¤šæ¨¡æ€å¤§æ¨¡å‹é‡è¦åº”ç”¨
Coming Soon...

#### å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹
Coming Soon...

## å¤§æ¨¡å‹é«˜æ•ˆä¼˜åŒ–ä¸æ‰©å±•
### å¤§æ¨¡å‹Scaling-Lawsä¸å®ç”¨æ¨å¹¿

+ `åŸºç¡€çŸ¥è¯†`:

> + **Scaling Laws** æ˜¯æŒ‡æ¨¡å‹æ€§èƒ½ï¼ˆå¦‚æŸå¤±ã€ç²¾åº¦ï¼‰ä¸æ¨¡å‹è§„æ¨¡ï¼ˆå¦‚å‚æ•°æ•°é‡ $N$ ã€è®­ç»ƒæ•°æ®é‡ $D$ ã€è®¡ç®—èµ„æº $C$ ï¼‰ä¹‹é—´çš„ç»éªŒæ€§å…³ç³»ã€‚å…³é”®è¦ç‚¹åŒ…æ‹¬ï¼š

> 1. **å‚æ•°æ•°é‡ $N$**ï¼šæ¨¡å‹æ€§èƒ½éšå‚æ•°å¢åŠ æå‡ï¼Œä½†å¢å¹…é€æ¸å‡å°ã€‚
> 2. **æ•°æ®é‡ $D$**ï¼šæ›´å¤šæ•°æ®èƒ½æå‡å¤§æ¨¡å‹æ€§èƒ½ã€‚
> 3. **è®¡ç®—é‡ $C$**ï¼šæ›´å¤šè®¡ç®—èµ„æºèƒ½å¸¦æ¥æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚
> 4. **æŸå¤±å‡½æ•° $L$**ï¼šéšç€èµ„æºæŠ•å…¥å¢åŠ ï¼ŒæŸå¤±å‡½æ•°å‘ˆç°æ¬¡å¹‚ç¼©å‡è¶‹åŠ¿ã€‚
> 5. **åº”ç”¨**ï¼šScaling Laws ä¸»è¦ç”¨äºæŒ‡å¯¼æ¨¡å‹çš„è®¾è®¡ä¸è®­ç»ƒï¼Œä½¿å¾—èµ„æºçš„æŠ•å…¥èƒ½å¤Ÿå¸¦æ¥æœ€ä½³çš„æ€§èƒ½æå‡ã€‚æ¯”å¦‚é€šè¿‡ Scaling Lawsï¼Œç ”ç©¶äººå‘˜å¯ä»¥é¢„ä¼°å¢åŠ å‚æ•°ã€æ•°æ®æˆ–è®¡ç®—æ˜¯å¦ä¼šæœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œä»è€Œæ›´åˆç†åœ°åˆ†é…èµ„æºï¼Œæ¨åŠ¨å¼€å‘æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚

> + Scaling Lawsä¸ä»…æ˜¯ä¸€ç§é’ˆå¯¹å¤§æ¨¡å‹è®­ç»ƒçš„ç†è®ºç§¯ç´¯ï¼Œè¿˜æ˜¯ä¸€ç§å®è·µæ–¹æ³•ï¼Œå¯ä»¥è¿ç§»åˆ°å„ç§éœ€è¦ä¼°è®¡æ•°æ®é‡ã€æ•°æ®æ¯”ä¾‹ã€å‚æ•°é‡ï¼ˆæˆ–å…¶ä»–æ¦‚å¿µï¼‰çš„å¤§æ¨¡å‹è®­ç»ƒåœºæ™¯ï¼ˆå³ï¼Œæ‰¾è‡ªå·±ç ”ç©¶åœºæ™¯é‡Œçš„Scaling Lawsï¼‰ã€‚

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:Scaling Lawsä¸å®ç”¨æ¨å¹¿**


| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **Scaling laws for neural language models** [[paper]](arxiv.org/abs/2001.08361) | Scaling Laws | OpenAIæŠ€æœ¯æŠ¥å‘Š | 2020-01-23 |
| **Training Compute-Optimal Large Language Models** [[paper]](https://arxiv.org/abs/2203.15556) | æœ‰å…³å¤§æ¨¡å‹å‚æ•°é‡ï¼Œæ•°æ®é‡å’Œè®¡ç®—é‡çš„Scaling Laws | NIPS2022 | 2022-03-29 |
| **Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies** [[paper]](https://arxiv.org/abs/2407.13623) [[project]](https://github.com/sail-sg/scaling-with-vocab) | æœ‰å…³å¤§æ¨¡å‹è¯è¡¨çš„Scaling Laws | arxiv | 2024-07-18 |
| **D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models** [[paper]](https://arxiv.org/abs/2406.01375) | Continual Pre-Trainingçš„Scaling Laws | arxiv | 2024-06-03 |
| **Observational Scaling Laws and the Predictability of Language Model Performance** [[paper]](https://arxiv.org/abs/2405.10938) [[project]](https://github.com/ryoungj/ObsScaling)| åœ¨å¤šä¸ªModel Familyï¼Œå¤šä¸ªBenchmarkä¸Šæ€»ç»“çš„é€šç”¨Scaling Laws | arxiv | 2024-05-17 |



### å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆè®­ç»ƒ

+ `åŸºç¡€çŸ¥è¯†`:

>  + **æ¦‚å¿µç®€ä»‹**:ç”±äºå¤§è¯­è¨€æ¨¡å‹å‚æ•°é‡ååˆ†åºå¤§ï¼Œå½“å°†å…¶åº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œå¾®è°ƒå…¨éƒ¨å‚æ•°éœ€è¦ç›¸å½“é«˜çš„ç®—åŠ›ã€‚ä¸ºäº†èŠ‚çœæˆæœ¬ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†å¤šç§å‚æ•°é«˜æ•ˆï¼ˆParameter Efficientï¼‰çš„å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨ä»…è®­ç»ƒå°‘é‡å‚æ•°ä½¿æ¨¡å‹é€‚åº”åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚

>  + **å‚æ•°é«˜æ•ˆæ–¹æ³•åˆ†ç±»**:å½“å‰åº”ç”¨æœ€é¢‘ç¹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å½“å±LoRAåŠå…¶å˜ä½“(å¦‚QLoRA,AdaLoRAç­‰)ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå…¸å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•è¿˜æœ‰åŸºäºAdapterçš„æ–¹æ³•(å¦‚[Adapter tuning](https://arxiv.org/pdf/1902.00751))å’ŒåŸºäºå‰ç¼€å¾®è°ƒçš„æ–¹æ³•(å¦‚[Prefix Tuning](https://arxiv.org/abs/2101.00190))


>  + **å…¸å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ä¸¾ä¾‹â€”â€”**[LoRA](https://arxiv.org/abs/2106.09685),æ˜¯å½“å‰è¢«å¹¿æ³›åº”ç”¨äºLLMè®­ç»ƒçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚LoRAæ–¹æ³•æµç¨‹ä¸º:å›ºå®šé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ä¸å˜ï¼Œåœ¨åŸæœ¬æƒé‡çŸ©é˜µæ—è·¯æ·»åŠ ä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ä½œä¸ºå¯è®­ç»ƒå‚æ•°(è§ä¸‹å›¾),ç”¨ä»¥æ¨¡æ‹Ÿæ¨¡å‹å‚æ•°çš„å˜åŒ–é‡ã€‚

>  + æ¨¡å‹å‚æ•°å˜åŒ–é‡çš„è®¡ç®—å…¬å¼ä¸º:  $W = W_0 + B \cdot A$  (å…¶ä¸­ $B$ , $A$ ä¸ºä½ç§©çŸ©é˜µã€‚åˆå§‹åŒ–æ—¶ï¼ŒçŸ©é˜µ $A$ é€šè¿‡é«˜æ–¯å‡½æ•°åˆå§‹åŒ–ï¼ŒçŸ©é˜µ $B$ ä¸ºé›¶åˆå§‹åŒ–ï¼Œä½¿å¾—è®­ç»ƒå¼€å§‹ä¹‹å‰æ—è·¯å¯¹åŸæ¨¡å‹ä¸é€ æˆå½±å“)

<div align="center">
  <img src="figures/LoRA.jpg" alt="LoRA" width="500"><br>
</div></br>

>  + LoRAåœ¨å®é™…å¾®è°ƒå¤§æ¨¡å‹æ—¶å…·æœ‰è‰¯å¥½æ•ˆæœï¼Œå¹¶ä¸”ç”±äºå‚æ•°åˆå¹¶ï¼Œä¸ä¼šå¸¦æ¥é¢å¤–æ¨ç†æ—¶å»¶ã€‚åç»­çš„ç›¸å…³å·¥ä½œä¹Ÿå°è¯•æ”¹è¿›LoRAï¼Œå°†å…¶åº”ç”¨åˆ°æ›´å¹¿æ³›çš„å®é™…ä»»åŠ¡ä¸­ã€‚

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:å¤ç”¨å’Œç»„åˆLoRAæ¨¡ç»„ for few/zero shots learning**


| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning** [[paper]](https://arxiv.org/abs/2205.05638)[[project]](https://github.com/r-three/t-few) | Adapter Tuning, å¤šä»»åŠ¡å­¦ä¹  | NIPS2022 | 2022-08-26 |
| **Combining Parameter-efficient Modules for Task-level Generalisation** [[paper]](https://aclanthology.org/2023.eacl-main.49/)[[project]](https://github.com/microsoft/mttl) | LoRAç»„åˆ, å¤šä»»åŠ¡å­¦ä¹  | EACL2023 | 2023 |
| **LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition** [[paper]](https://arxiv.org/abs/2307.13269)[[project]](https://huggingface.co/lorahub) | LoRAç»„åˆ, å¤šä»»åŠ¡å­¦ä¹  | COLM2024 | 2023-07-25 |
| **Towards Modular LLMs by Building and Reusing a Library of LoRAs** [[paper]](https://arxiv.org/abs/2405.11157)| LoRAç»„åˆ, å¤šä»»åŠ¡å­¦ä¹  | ICML2024 | 2024-05-18 |
| **Learning to Route Among Specialized Experts for Zero-Shot Generalization** [[paper]](https://arxiv.org/abs/2402.05859) [[project]](https://github.com/r-three/phatgoose)| LoRAç»„åˆ, å¤šä»»åŠ¡å­¦ä¹  | ICML2024 | 2024-02-08 |


### å¤§æ¨¡å‹æ˜¾å­˜é«˜æ•ˆè®­ç»ƒ
Coming Soon...

### å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ

+ `åŸºç¡€çŸ¥è¯†`:

>  + **æ¦‚å¿µç®€ä»‹**:å¤§è¯­è¨€æ¨¡å‹çš„è½åœ°åº”ç”¨å—åˆ°å…¶è¾ƒå¤§çš„æ¨ç†å¼€é”€çš„é™åˆ¶ï¼Œå¯¹éƒ¨ç½²èµ„æºã€ç”¨æˆ·ä½“éªŒã€ç»æµæˆæœ¬éƒ½å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œå°†LLaMA-2-70Bæ¨¡å‹è¿›è¡Œéƒ¨ç½²æ¨ç†ï¼Œè‡³å°‘éœ€è¦6å¼ RTX 3090Tiæ˜¾å¡æˆ–2å¼ NVIDIA A100æ˜¾å¡ï¼Œä»¥éƒ¨ç½²åœ¨A100æ˜¾å¡ä¸Šä¸ºä¾‹ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆ512é•¿åº¦çš„è¯å—ï¼ˆtokenï¼‰åºåˆ—éœ€è¦è€—æ—¶è¶…è¿‡50ç§’ã€‚å› æ­¤ï¼Œè®¾è®¡ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼€é”€çš„æŠ€æœ¯ï¼Œæˆä¸ºè®¸å¤šç ”ç©¶çš„é‡è¦ç›®æ ‡ã€‚

>  + å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…éƒ¨ç½²åº”ç”¨ä¸­ï¼Œäººä»¬é€šå¸¸å…³æ³¨å…¶**å»¶æ—¶ã€ååã€åŠŸè€—å’Œå­˜å‚¨**ï¼Œè€Œåœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸‰ä¸ªé‡è¦å› ç´ ä¼šç›´æ¥å½±å“ä¸Šè¿°æ•ˆç‡æŒ‡æ ‡ï¼Œåˆ†åˆ«æ˜¯**è®¡ç®—å¼€é”€ï¼ˆComputational Costï¼‰**ã€**è®¿å­˜å¼€é”€ï¼ˆMemory Access Costï¼‰**å’Œ**å­˜å‚¨å¼€é”€ï¼ˆMemory Costï¼‰**ã€‚[ç°æœ‰ç»¼è¿°](https://arxiv.org/abs/2404.14294)æ€»ç»“å‡ºå½±å“ä¸Šè¿°æŒ‡æ ‡çš„ä¸‰ç‚¹æ ¹æœ¬å› ç´ ï¼Œåˆ†åˆ«ä¸ºï¼š

> 1. **æ¨¡å‹è§„æ¨¡**ï¼šä¸»æµå¤§è¯­è¨€æ¨¡å‹åºå¤§çš„æ¨¡å‹è§„æ¨¡ä¼šå¯¼è‡´å·¨å¤§çš„è®¡ç®—é‡ã€è®¿å­˜é‡å’Œå­˜å‚¨é‡ã€‚
> 2. **æ³¨æ„åŠ›ç®—å­**ï¼šä½œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒç®—å­ï¼Œæ³¨æ„åŠ›ç®—å­å…·æœ‰ä¸è¾“å…¥é•¿åº¦å‘ˆå¹³æ–¹å…³ç³»å¢é•¿çš„è®¡ç®—å’Œå­˜å‚¨å¤æ‚åº¦ã€‚
> 3. **è§£ç æ–¹å¼**ï¼šä¸»æµçš„è‡ªå›å½’è§£ç æ–¹å¼å¯¼è‡´æä½çš„è®¡ç®—-è®¿å­˜æ¯”å’Œç¡¬ä»¶åˆ©ç”¨ç‡ï¼ŒåŒæ—¶åŠ¨æ€å¢é•¿çš„KV cache[(å¦‚ä¸äº†è§£KV cacheï¼Œå¯å‚è€ƒæ­¤ç®€ä»‹)](https://zhuanlan.zhihu.com/p/662498827)ä¼šå¯¼è‡´ç¢ç‰‡åŒ–çš„å†…å­˜ä½¿ç”¨ï¼Œå¯¹è®¿å­˜å¼€é”€å’Œå­˜å‚¨å¼€é”€å¸¦æ¥å¢é•¿ã€‚

>  + é’ˆå¯¹ä¸Šè¿°å› ç´ ï¼Œä»æ¨ç†é«˜æ•ˆè§’åº¦å‡ºå‘çš„ç°æœ‰æŠ€æœ¯å¯å¤§è‡´åˆ†ä¸ºä¸‰ç±»[(åˆ†ç±»ä¾æ®)](https://arxiv.org/abs/2404.14294):

> 1. **æ•°æ®å±‚ä¼˜åŒ–æŠ€æœ¯**ï¼šï¼ˆæŠ€æœ¯å…¸å‹:Promptå‹ç¼©ï¼‰æŒ‡é€šè¿‡ä¼˜åŒ–è¾“å…¥æç¤ºè¯æˆ–è§„åˆ’æ¨¡å‹è¾“å‡ºå†…å®¹ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚è¿™ç±»ä¼˜åŒ–æŠ€æœ¯é€šå¸¸ä¸éœ€è¦ä¿®æ”¹æ¨¡å‹æœ¬èº«ï¼Œå› æ­¤é¿å…äº†å¤§é‡çš„æ¨¡å‹è®­ç»ƒæˆ–å¾®è°ƒå¼€é”€ï¼›
> 2. **æ¨¡å‹å±‚ä¼˜åŒ–æŠ€æœ¯**ï¼šï¼ˆæŠ€æœ¯å…¸å‹:æ¨¡å‹å‰ªæï¼‰æŒ‡é€šè¿‡è®¾è®¡é«˜æ•ˆçš„æ¨¡å‹ç»“æ„æˆ–æ¨¡å‹å‹ç¼©æŠ€æœ¯ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚è¿™ç±»æŠ€æœ¯é€šå¸¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒæˆ–å¾®è°ƒæ¥æ¢å¤ä»»åŠ¡ç²¾åº¦ï¼ŒåŒæ—¶é€šå¸¸å¯¹è¾“å‡ºç»“æœæ˜¯æœ‰æŸçš„ï¼›
> 3. **ç³»ç»Ÿå±‚ä¼˜åŒ–æŠ€æœ¯**ï¼šï¼ˆæŠ€æœ¯å…¸å‹:ç®—å­ä¼˜åŒ–ï¼ŒçŒœæµ‹è§£ç ï¼‰æŒ‡é€šè¿‡ä¼˜åŒ–æ¨ç†å¼•æ“æˆ–æœåŠ¡ç³»ç»Ÿä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚è¿™ç±»æŠ€æœ¯é€šå¸¸ä¸éœ€è¦é¢å¤–çš„æ¨¡å‹è®­ç»ƒå¼€é”€ï¼ŒåŒæ—¶å¯ä»¥ä¿è¯å¯¹è¾“å‡ºç»“æœæ˜¯æ— æŸçš„ã€‚

<div align="center">
  <img src="figures/LLM_efficient_inference.png" alt="LLM_efficient_inference" width="500"><br>
</div></br>

> æ›´å¤šåŸºç¡€çŸ¥è¯†å¯è§äº[(å‚è€ƒç»¼è¿°)](https://arxiv.org/abs/2404.14294)

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:ä½¿ç”¨è¶…ç½‘ç»œç”ŸæˆPEFTæ¨¡å—(æ¶‰åŠå†…å®¹å½’å±ï¼šæ•°æ®å±‚ä¼˜åŒ–æŠ€æœ¯)**
> + **è¯é¢˜ç®€ä»‹**:è¶…ç½‘ç»œï¼ˆHypernetworksï¼‰è¡¨ç¤ºç”¨äºäº§ç”Ÿç½‘ç»œå‚æ•°çš„ç½‘ç»œã€‚LLMæ¨ç†é€Ÿåº¦å—é™äºå†—é•¿çš„æŒ‡ä»¤å’Œå°‘æ ·æœ¬ç¤ºä¾‹ã€‚ç”¨è¶…ç½‘ç»œä¸ºinstruction/few-shot demonstrationç”ŸæˆPEFTæ¨¡å—,åˆ™æ— éœ€æ¯æ¬¡å¤„ç†è¾“å…¥promptã€‚ï¼ˆéœ€æ³¨æ„ï¼Œæ­¤è¯é¢˜ä¸‹çš„æŠ€æœ¯å¯ä»¥æŒ‰å¤šç§ç†è®ºè§†è§’è§£è¯»ï¼Œæ—¢å¯ä»¥è§†ä¸ºä¸€ç§èƒ½å¢ç›Šå¤§æ¨¡å‹æ¨ç†æ•ˆç‡çš„soft promptæŠ€æœ¯ï¼Œä¹Ÿå¯ä»¥è§†ä¸ºä¸€ç§å‚æ•°é«˜æ•ˆçš„å¤§æ¨¡å‹ä¼˜åŒ–æ–¹æ³•ã€‚ï¼‰

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **HyperPrompt: Prompt-based Task-Conditioning of Transformers** [[paper]](https://arxiv.org/abs/2203.00759) | å¤šä»»åŠ¡å­¦ä¹  | ICML2022 | 2022-03-01 |
| **Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning** [[paper]](https://arxiv.org/abs/2310.11670)[[project]](https://github.com/Bumble666/PHA) | å¤šä»»åŠ¡å­¦ä¹  | EMNLP2023 | 2023-10-18 |
| **HyperTuning: Toward Adapting Large Language Models without Back-propagation** [[paper]](https://arxiv.org/abs/2211.12485)| å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆè®­ç»ƒ | ICML2023 | 2022-11-22 |
| **HINT: Hypernetwork Instruction Tuning for Efficient Zero- & Few-Shot Generalisation** [[paper]](https://arxiv.org/abs/2212.10315)[[project]](https://github.com/allenai/hyper-task-descriptions) | å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ | ACL2023 | 2022-12-20 |

+ **è¯é¢˜:ä»ç¨€ç–æ€§è§’åº¦çœ‹LLMæ¨ç†åŠ é€Ÿ(æ¶‰åŠå†…å®¹å½’å±ï¼šæ¨¡å‹å±‚ä¼˜åŒ–æŠ€æœ¯ï¼Œç³»ç»Ÿå±‚ä¼˜åŒ–æŠ€æœ¯)**

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time** [[paper]](https://arxiv.org/abs/2310.17157)[[project]](https://github.com/FMInference/DejaVu) | å¤§æ¨¡å‹æ¨ç†é«˜æ•ˆ | ICML2023 | 2023-10-26 |
| **Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning** [[paper]](https://arxiv.org/abs/2310.06694)[[project]](https://github.com/princeton-nlp/LLM-Shearing) | å¤§æ¨¡å‹å‰ªæ | ICLR2024 | 2023-10-10 |
| **Fluctuation-based Adaptive Structured Pruning for Large Language Models** [[paper]](https://arxiv.org/abs/2312.11983)[[project]](https://github.com/CASIA-IVA-Lab/FLAP) | å¤§æ¨¡å‹å‰ªæ | AAAI2024 | 2023-12-19 |
| **PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs** [[paper]](https://arxiv.org/abs/2312.15230)[[project]](https://github.com/ZIB-IOL/PERP) | å¤§æ¨¡å‹å‰ªæ | arxiv | 2023-12-23 |
| **LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training** [[paper]](https://arxiv.org/abs/2406.16554)[[project]](https://github.com/pjlab-sys4nlp/llama-moe) | LLM MoE | arxiv | 2024-06-24 |


## å¤§æ¨¡å‹çŸ¥è¯†å¤„ç†
### å¤§æ¨¡å‹çŸ¥è¯†æ¨ç†
Coming Soon...

### å¤§æ¨¡å‹çŸ¥è¯†ç¼–è¾‘
Coming Soon...

### å¤§æ¨¡å‹çŸ¥è¯†è’¸é¦
Coming Soon...

### å¤§æ¨¡å‹å¯è§£é‡Šæ€§
Coming Soon...

### å¤§æ¨¡å‹å¹»è§‰

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:LLMçš„å¹»è§‰åˆ†æã€å¸¸è¯†ç†è§£ä¸æŒ‡ä»¤å¾®è°ƒåˆ†æ**

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **On Large Language Models' Hallucination with Regard to Known Facts** [[paper]](https://arxiv.org/abs/2403.20009) | LLMå¹»è§‰åˆ†æ | NAACL2024 | 2024-03-29 |
| **Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method** [[paper]](https://arxiv.org/abs/2310.17918) | LLMå¹»è§‰åˆ†æ | NAACL2024 | 2023-10-27 |
| **R-Tuning: Instructing Large Language Models to Say 'I Don't Know'** [[paper]](https://arxiv.org/abs/2311.09677) | LLMå¹»è§‰åˆ†æ | NAACL2024 | 2023-11-16 |
| **ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models** [[paper]](https://arxiv.org/abs/2303.16421) | LLMå¸¸è¯†ç†è§£ | COLING2024 | 2023-03-29 |
| **Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge** [[paper]](https://arxiv.org/abs/2305.05976) [[project]](https://github.com/jiangjiechen/uncommongen) | LLMå¸¸è¯†ç†è§£ | ACL2023 | 2023-05-10 |
| **Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense** [[paper]](https://arxiv.org/abs/2405.04655) | LLMå¸¸è¯†ç†è§£ | NAACL2024 | 2024-05-07 |
| **From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning** [[paper]](https://arxiv.org/abs/2310.00492) [[project]](https://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs) | LLMæŒ‡ä»¤å¾®è°ƒ | NAACL2024 | 2023-09-30 |


## å¤§æ¨¡å‹çš„æ–‡æœ¬å¤„ç†ä¸å¯¹é½
### å¤§æ¨¡å‹çš„è¡¨æ ¼å¤„ç†
> **è¡¨æ ¼**ä½œä¸ºä¸€ç§ç»“æ„åŒ–æ–‡æœ¬å†…å®¹ï¼Œè¢«å¹¿æ³›åº”ç”¨äºå­˜å‚¨å®é™…ç”Ÿäº§ä¸­çš„å„é¡¹é‡è¦æ•°æ®ä¿¡æ¯ã€‚ä¸ºäº†è·å–è§„æ¨¡åºå¤§åˆæå¯Œä»·å€¼çš„è¡¨æ ¼æ•°æ®ï¼Œå·¥ä¸šç•Œå¼€å§‹é€æ­¥æ¨åŠ¨è¡¨æ ¼æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ã€‚

> **è¡¨æ ¼æ™ºèƒ½ï¼ˆTable Intelligenceï¼‰æŠ€æœ¯**æ—¨åœ¨é€šè¿‡è‡ªåŠ¨ç†è§£ã€å¤„ç†å’Œåˆ†æè¡¨æ ¼æ•°æ®æ¥æ»¡è¶³ä¸åŒçš„ç”¨æˆ·éœ€æ±‚ã€‚è¡¨æ ¼æ™ºèƒ½ä¼šæ¶‰åŠåˆ°å¤šç§ä»»åŠ¡ï¼Œæ¯”å¦‚æœ€ä¼ ç»Ÿçš„è¡¨æ ¼é—®ç­”ã€å’Œæ•°æ®åº“å¯†åˆ‡ç›¸å…³çš„NL2SQLã€è¡¨æ ¼å¤„ç†ï¼ˆå¢åˆ æ”¹æŸ¥ï¼‰ã€é«˜é˜¶æ•°æ®åˆ†æï¼ˆBIéœ€æ±‚ï¼‰ç­‰ã€‚åœ¨è¿™äº›æŠ€æœ¯ä¸­ï¼ŒåŸºäºå¤§æ¨¡å‹çš„è¡¨æ ¼æ™ºèƒ½æŠ€æœ¯æ˜¯å½“å‰çš„å…³æ³¨çƒ­ç‚¹ã€‚

> åŸºäºå¤§æ¨¡å‹çš„è¡¨æ ¼æ™ºèƒ½æŠ€æœ¯åŒ…å«å¤šä¸ªæ–¹å‘ï¼Œæ¯”å¦‚é€šè¿‡æç¤ºå¤§æ¨¡å‹(Prompting)çš„æ–¹å¼å®Œæˆè¡¨æ ¼ä»»åŠ¡ï¼Œè®­ç»ƒä¸“é—¨ç†è§£è¡¨æ ¼æ•°æ®çš„å¤§æ¨¡å‹(Training)ç­‰ã€‚æœ¬éƒ¨åˆ†å°†æ€»ç»“å¹¶å±•ç¤ºå„ä¸ªæ–¹å‘ä¸‹çš„éƒ¨åˆ†è®ºæ–‡

> æ›´å¤šæ–‡çŒ®å¯è§äº[Awesome-Tabular-LLMs](https://github.com/SpursGoZmy/Awesome-Tabular-LLMs)

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:é¢å‘è¡¨æ ¼æ™ºèƒ½çš„AgentæŠ€æœ¯**

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **API-Assisted Code Generation for Question Answering on Varied Table Structures** [[paper]](https://arxiv.org/abs/2310.14687) | é’ˆå¯¹å¤šç§è¡¨æ ¼ç»“æ„çš„Promptæ–¹æ³• | EMNLP2023 | 2023-10-23 |
| **TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning** [[paper]](https://arxiv.org/abs/2312.09039) | è€ƒè™‘äº†é•¿è¡¨æ ¼åœºæ™¯çš„è¡¨æ ¼Promptå¤„ç†æ–¹æ³• | arxiv | 2023-12-14 |
| **SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models** [[paper]](https://arxiv.org/abs/2305.19308) [[project]](https://github.com/BraveGroup/SheetCopilot) | é¦–ä¸ªå¤„ç†Excelè¡¨æ ¼çš„Agentæ¡†æ¶ | NIPS2023 | 2023-05-30 |
| **SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models** [[paper]](https://arxiv.org/abs/2403.03636) | å…³æ³¨å¤šæ­¥æ¨ç†è¡¨æ ¼ä»»åŠ¡çš„Agentæ¡†æ¶ | ICML2024 | 2024-03-06 |




### å¤§æ¨¡å‹çš„é•¿æ–‡æœ¬å¤„ç†
Coming Soon...

### å¤§æ¨¡å‹çš„äººç±»åå¥½å¯¹é½
Coming Soon...

### å¤§æ¨¡å‹æ–‡æœ¬æ°´å°æŠ€æœ¯

è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å¿«é€Ÿç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›ä¹Ÿå¸¦æ¥äº†**ä¿¡æ¯ä¼ æ’­**å’Œ**çŸ¥è¯†äº§æƒ**æ–¹é¢çš„æŒ‘æˆ˜ã€‚æ–‡æœ¬æ°´å°æŠ€æœ¯é€šè¿‡**åµŒå…¥å¯è¯†åˆ«çš„æ ‡è®°**æ¥å®ç°**å†…å®¹è¿½è¸ª**å’Œ**æ¥æºå½’å±**ï¼Œæ˜¯è§£å†³å¤§è¯­è¨€æ¨¡å‹æ»¥ç”¨é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚

---
`æ¥è‡ªChatGPTçš„ä¾‹å­`


åœ¨ç”Ÿæˆå¼æ–‡æœ¬ä¸­åµŒå…¥æ°´å°çš„æŠ€æœ¯å¾€å¾€éšè”½ä¸”å¤æ‚ï¼Œé€šå¸¸ä¸ä¼šå¯¹æ–‡æœ¬çš„è¡¨é¢å«ä¹‰äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚å› æ­¤ï¼Œæˆ‘å¯ä»¥é€šè¿‡ç®€å•çš„ä¾‹å­æ¼”ç¤ºå¯èƒ½çš„å·®å¼‚ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®é™…çš„æ°´å°åµŒå…¥å’Œæ£€æµ‹é€šå¸¸éœ€è¦æ›´å¤æ‚çš„ç®—æ³•ã€‚

### ç¤ºä¾‹ 1ï¼šæ²¡æœ‰åµŒå…¥æ°´å°çš„æ–‡æœ¬

```plaintext
The weather today is sunny with a slight breeze, perfect for a walk in the park.
```

### ç¤ºä¾‹ 2ï¼šåµŒå…¥æ°´å°çš„æ–‡æœ¬

```plaintext
The weather today is bright with a slight breeze, making it ideal for a walk in the park.
```

### è§£é‡Šï¼š
- **æ— æ°´å°æ–‡æœ¬**ï¼ˆç¤ºä¾‹ 1ï¼‰ï¼šå®Œå…¨è‡ªç„¶ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæ²¡æœ‰ç»è¿‡ä»»ä½•ç‰¹æ®Šå¤„ç†ã€‚
- **æœ‰æ°´å°æ–‡æœ¬**ï¼ˆç¤ºä¾‹ 2ï¼‰ï¼šåœ¨ä¸æ”¹å˜æ•´ä½“è¯­ä¹‰çš„å‰æä¸‹ï¼ŒæŸäº›è¯è¯­è¢«æ›¿æ¢ã€‚ä¾‹å¦‚ï¼Œ`sunny` è¢«æ›¿æ¢ä¸º `bright`ï¼Œ`perfect` è¢«æ›¿æ¢ä¸º `ideal`ã€‚è¿™äº›å¾®å°çš„å˜åŒ–å¯ä»¥è¢«ç‰¹å®šçš„æ°´å°ç®—æ³•è¯†åˆ«ï¼Œå¸®åŠ©ç¡®å®šè¯¥æ–‡æœ¬æ˜¯å¦æºè‡ªç‰¹å®šæ¨¡å‹ã€‚
---

> æ›´å¤šåŸºç¡€çŸ¥è¯†å¯è§äº[(å‚è€ƒç»¼è¿°)](https://arxiv.org/abs/2312.07913)

> æ›´å¤šæ–‡æœ¬æ°´å°å·¥å…·å¯è§äº[(MarkLLM)](https://github.com/THU-BPM/MarkLLM)

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æ°´å°**

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models** [[paper]](https://arxiv.org/abs/2308.14401) | æ–‡æœ¬æ°´å° | FSE2023 | 2023-08-28 |
| **A Watermark for Large Language Models** [[paper]](https://arxiv.org/abs/2301.10226)[[project]](https://github.com/jwkirchenbauer/lm-watermarking) | å¤§æ¨¡å‹æ–‡æœ¬æ°´å° | ICML2023 | 2023-01-24 |
| **Towards Codable Watermarking for Injecting Multi-bits Information to LLMs** [[paper]](https://arxiv.org/abs/2307.15992)[[project]](https://github.com/lancopku/codable-watermarking-for-llm) | å¤§æ¨¡å‹æ–‡æœ¬æ°´å° | ICLR2024 | 2023-07-29 |
| **UNBIASED WATERMARK FOR LARGE LANGUAGE MODELS** [[paper]](https://arxiv.org/abs/2310.10669)[[project]](https://github.com/xiaoniu-578fa6bff964d005/UnbiasedWatermark) | å¤§æ¨¡å‹æ–‡æœ¬æ°´å° | **ICLR2024 Spotlight** | 2023-09-22 |
| **SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation** [[paper]](https://arxiv.org/abs/2310.03991)[[project]](https://github.com/bohanhou14/SemStamp) | å¤§æ¨¡å‹æ–‡æœ¬æ°´å° | NAACL2024 | 2023-10-06 |
| **Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models** [[paper]](https://arxiv.org/abs/2311.04378)[[project]](https://hanlin-zhang.com/impossibility-watermarks/) | å¤§æ¨¡å‹æ–‡æœ¬æ°´å° | ICML2024 | 2023-11-07 |


## å¤§æ¨¡å‹æ¶æ„
> ç›®å‰å¤§æ¨¡å‹çš„ä¸»æµæ¶æ„å‡å‚è€ƒTransformerå®ç°ã€‚ç„¶è€Œï¼ŒTransformerçš„æ¶æ„è®¾è®¡æœ¬èº«å­˜åœ¨ä¸è¶³ã€‚æ¯”å¦‚ï¼ŒTransformeræ¶æ„å†…æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦è¿‡é«˜ï¼Œä¼šé™åˆ¶å…¶åœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚å½“å‰ï¼Œéƒ¨åˆ†å·¥ä½œï¼ˆå¦‚Mambaï¼‰å°è¯•ä»æ¨¡å‹æ¶æ„å‡ºå‘ï¼Œç¼“è§£è¿™ä¸€é—®é¢˜ã€‚

<font size=4><center><b> è®ºæ–‡åˆ—è¡¨ </b> </center></font>

+ **è¯é¢˜:Mambaæ¨¡å‹ï¼šçŠ¶æ€ç©ºé—´æ¨¡å‹çš„å‰ä¸–ä»Šç”Ÿ**

| æ ‡é¢˜ | ç±»å‹ | ä¼šè®®  | æ—¥æœŸ |
| ------ | :---: | :---: | :---: |
| **Mamba: Linear-Time Sequence Modeling with Selective State Spaces** [[paper]](https://arxiv.org/abs/2312.00752)[[project]](https://github.com/state-spaces/mamba) | å¤§æ¨¡å‹æ¶æ„ | arxiv | 2023-12-01 |
| **Jamba: A Hybrid Transformer-Mamba Language Model** [[paper]](https://arxiv.org/abs/2403.19887)[[project]](https://www.ai21.com/jamba) | å¤§æ¨¡å‹æ¶æ„ | arxiv | 2024-03-28 |
| **Jamba-1.5: Hybrid Transformer-Mamba Models at Scale** [[paper]](https://arxiv.org/abs/2408.12570)[[project]](https://huggingface.co/ai21labs) | å¤§æ¨¡å‹æ¶æ„ | arxiv | 2024-08-22 |


